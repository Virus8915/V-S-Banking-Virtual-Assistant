# -*- coding: utf-8 -*-
"""Copy of Banking Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yYtZpRf0rOnFltHyzhyqm0k-epqqzbI5
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
import re
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics.pairwise import cosine_similarity

df1 = pd.read_csv("BankFAQs.csv")
df1.head()

df1.info()

df1.isna().sum()

df1.head()

df2 = pd.read_csv("Bank_faq.csv")
df2.head()

df = pd.concat([df1, df2], axis =0)

df.info()

"""Exploratory Data Analysis"""

plt.figure(figsize=(10,7))
sb.set(font_scale=1)
sb.set_style('whitegrid')
sb.histplot(df['Class'])

"""•	Maximum Query arises in Insurance Class while least query comes under Fund Transfer Section.
•	Cards, Account and Loan Enquiries are also high enough.
•	Fewer Enquiries can be seen in Investments and Security.
"""

plt.figure(figsize=(20,14))
df['Class'].value_counts().plot(kind = 'pie', autopct='%1.1f%%')

26.2+22.6+21.2+18.2

"""•	For Insurance 26.2% enquiries, For Cards 22.6% enquiries, For Loans 21.2% enquiries and For Accounts 18.2% enquiries are       there.
•	Security and Fund Transfer are having combined enquiry of 4.1% only.
•	More than 88% enquiries are under 4 classes out of 7 classes.

"""

df.head()

import string
string.punctuation

def remove_punc(txt):
    txt_wo_punct = "".join([i for i in txt if i not in string.punctuation])
    return txt_wo_punct

df['Question'] = df['Question'].apply(lambda x: remove_punc(x))
df['Answer'] = df['Answer'].apply(lambda x: remove_punc(x))
df.head()

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

nltk.download('wordnet')

df['Question'][:5]

wnl = WordNetLemmatizer()

def clean(content):
    word_tok = nltk.word_tokenize(content)
    cleaned_words = [word for word in word_tok if word not in stop_words]
    cleaned_words = [wnl.lemmatize(word) for word in cleaned_words]
    return ' '.join(cleaned_words)

questions = df['Question'].values

import nltk
nltk.download('punkt')

X = []

for question in questions:
    X.append(clean(question))

X[:15]

tf = TfidfVectorizer()
X = tf.fit_transform(X)

X.shape

le = LabelEncoder()
le.fit(df['Class'])

y = le.fit_transform(df['Class'])

y.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=500)

model_params= {
    'svm':{
        'model':SVC(gamma='auto'),
        'params':{
            'C': [1,10,20],
            'kernel': ['rbf', 'linear']
        }
    },
    'random_forest':{
        'model':RandomForestClassifier(),
        'params':{
            'n_estimators': [1,5,10]
        }
    },
    'logistic_regression':{
        'model':LogisticRegression(solver='liblinear', multi_class='auto'),
        'params':{
            'C': [1,5,10]
        }
    }
    
}

scores = []

for model_name, mp in model_params.items():
    clf = GridSearchCV(mp['model'], mp['params'], cv = 5, return_train_score=False)
    clf.fit(X, y)
    scores.append({
        'model': model_name,
        'best_score':clf.best_score_,
        'best_params' :clf.best_params_
    })

df_best_score = pd.DataFrame(scores, columns=['model','best_score','best_params' ])
df_best_score

final_model = SVC(kernel='linear', C=1.0, gamma='auto'  )
final_model.fit(X_train, y_train)

final_model.score(X_test, y_test)

class_=le.inverse_transform(final_model.predict(X))

class_

import numpy as np

final_model.predict(X_test)

from sklearn.metrics.pairwise import cosine_similarity
user_question = "How to open savings account"
## Create a TF-IDF vectorizer to convert the text data and query to a vector representation


# Get the vector representationthe question and answer
answer_vectors = tf.transform(df['Answer']).toarray()
test_vector = tf.transform([user_question]).toarray()
# Calculate the cosine similarity between both vectors
cosine_sims = cosine_similarity(answer_vectors, test_vector)
# Get the index of the most similar text to the query
most_similar_idx = np.argmax(cosine_sims)
# Print the most similar text as the answer to the query
print(df.iloc[most_similar_idx]['Answer'])

def get_response(usrText):

    while True:

        if usrText.lower() == "bye":
            return "Bye"

        GREETING_INPUTS = ["hello", "hi", "greetings", "sup", "what's up", "hey","hiii","hii","yo"]

        a = [x.lower() for x in GREETING_INPUTS]

        sd=["Thanks","Welcome"]

        d = [x.lower() for x in sd]


        am=["OK"]

        c = [x.lower() for x in am]

        t_usr = tf.transform([clean(usrText.strip().lower())])
        class_ = le.inverse_transform(final_model.predict(t_usr))

        questionset = df[df['Class'].values == class_]

        cos_sims = []
        for question in questionset['Question']:
            sims = cosine_similarity(tf.transform([question]), t_usr)

            cos_sims.append(sims)

        ind = cos_sims.index(max(cos_sims))

        b = [questionset.index[ind]]

        if usrText.lower() in a:

            return ("Hi, I'm Emily!\U0001F60A")


        if usrText.lower() in c:
            return "Ok...Alright!\U0001F64C"

        if usrText.lower() in d:
            return ("My pleasure! \U0001F607")

        if max(cos_sims) > [[0.]]:
            a = df['Answer'][questionset.index[ind]]+"   "
            return a


        elif max(cos_sims)==[[0.]]:
           return "sorry! \U0001F605"

!pip install streamlit

import streamlit as st
import os

st.title("V&S Banking Virtual Assistant")

st.write("Hi,How can I help you?")

user_input = st.text_input("You: ")

if user_input:
    response = get_response(user_input)

    st.write(response)

